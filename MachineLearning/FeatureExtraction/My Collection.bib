@article{Wu2013,
abstract = {Feedback-driven program optimization (FDO) is common in modern compilers, including Just-In-Time compilers increasingly adopted for object-oriented or scripting languages. This paper describes a systematic study in understanding and alleviating the effects of sampling errors on the usefulness of the obtained profiles for FDO. Taking a statistical approach, it offers a series of counter-intuitive findings, and identifies two kinds of profile errors that affect FDO critically, namely zero-count errors and inconsistency errors. It further proposes statistical profile rectification, a simple approach to correcting profiling errors by leveraging statistical patterns in a profile. Experiments show that the simple approach enhances the effectiveness of sampled profile-based FDO dramatically, increasing the average FDO speedup from 1.16X to 1.3X, around 92% of what full profiles can yield. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Wu, Bo and Zhou, Mingzhou and Shen, Xipeng and Gao, Yaoqing and Silvera, Raul and Yiu, Graham},
doi = {10.1007/978-3-642-39038-8_27},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/wrappersPrint.pdf:pdf},
isbn = {9783642390371},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {classification,feature selection,filter,wrapper},
number = {97},
pages = {654--678},
title = {{Simple profile rectifications go a long way statistically exploring and alleviating the effects of sampling errors for program optimizations}},
volume = {7920 LNCS},
year = {2013}
}
@article{Vafaie1992,
abstract = {This paper describes an approach being explored to improve the usefulness of machine learning techniques for generating classification rules for complex, real world data. The approach involves the use of genetic algorithms as a "front end" to traditional rule induction systems in order to identify and select the best subset of features to be used by the rule induction system. This approach has been implemented and tested on difficult texture classification problems. The results are encouraging and indicate significant advantages to the presented approach in this domain.},
author = {Vafaie, H. and {De Jong}, K.},
doi = {10.1109/TAI.1992.246402},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/TAI92.pdf:pdf},
isbn = {0818629053},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
pages = {200--203},
title = {{Genetic algorithms as a tool for feature selection in machine learning}},
volume = {1992-Novem},
year = {1992}
}
@article{Khalid2014,
abstract = {Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.},
author = {Khalid, Samina and Khalil, Tehmina and Nasreen, Shamila},
doi = {10.1109/SAI.2014.6918213},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/SurveyofFeatureExtractionandFeatureSelectionTechniquesinMachineLearning.pdf:pdf},
isbn = {9780989319317},
journal = {Proceedings of 2014 Science and Information Conference, SAI 2014},
keywords = {Age Related Macula Degeneration (AMD),Correlation Based Method,FSA's,Feature Extraction/Transformation,Feature Selection,Feature Subset Selection,ICA,PCA,RELIEF},
pages = {372--378},
title = {{A survey of feature selection and feature extraction techniques in machine learning}},
year = {2014}
}
@article{Strobl2008,
abstract = {MOTIVATION: Pre-selection of informative features for supervised classification is a crucial, albeit delicate, task. It is desirable that feature selection provides the features that contribute most to the classification task per se and which should therefore be used by any classifier later used to produce classification rules. In this article, a conceptually simple but computer-intensive approach to this task is proposed. The reliability of the approach rests on multiple construction of a tree classifier for many training sets randomly chosen from the original sample set, where samples in each training set consist of only a fraction of all of the observed features. RESULTS: The resulting ranking of features may then be used to advantage for classification via a classifier of any type. The approach was validated using Golub et al. leukemia data and the Alizadeh et al. lymphoma data. Not surprisingly, we obtained a significantly different list of genes. Biological interpretation of the genes selected by our method showed that several of them are involved in precursors to different types of leukemia and lymphoma rather than being genes that are common to several forms of cancers, which is the case for the other methods. AVAILABILITY: Prototype available upon request.},
author = {Strobl and Zeileis},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Strobl+Zeileis-2008.pdf:pdf},
isbn = {9783790820836},
issn = {1460-2059},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Biological,Biological: analysis,Gene Expression Profiling,Gene Expression Profiling: methods,Humans,Monte Carlo Method,Neoplasm Proteins,Neoplasm Proteins: analysis,Neoplasms,Neoplasms: metabolism,Pattern Recognition,Tumor Markers},
number = {1},
pages = {110--7},
pmid = {18048398},
title = {{Danger: High Power! – Exploring the Statistical Properties of a Test for Random Forest Variable Importance}},
volume = {24},
year = {2008}
}
@article{Bolon-Canedo2013,
abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets. {\textcopyright} 2012 Springer-Verlag London Limited.},
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and S{\'{a}}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo},
doi = {10.1007/s10115-012-0487-8},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/s10115-012-0487-8.pdf:pdf},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Embedded methods,Feature selection,Filters,Synthetic datasets,Wrappers},
number = {3},
pages = {483--519},
title = {{A review of feature selection methods on synthetic data}},
volume = {34},
year = {2013}
}
@article{Molina2002,
abstract = {In view of the substantial number of existing feature selection algorithms, the need arises to count on criteria that enables to adequately decide which algorithm to use in certain situations. This work reviews several fundamental algorithms found in the literature and assesses their performance in a controlled scenario. A scoring measure ranks the algorithms by taking into account the amount of relevance , irrelevance and redundance on sample data sets. This measure computes the degree of matching between the output given by the algorithm and the known optimal solution. Sample size effects are also studied.},
author = {Molina, Luis Carlos L.C. and Belanche, Llu{\'{i}}s and Nebot, A. and Nebot, {\`{A}}ngela and Girona, Jordi and C, Campus Nord},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/R02-62.pdf:pdf},
isbn = {0-7695-1754-4},
issn = {0769517544},
journal = {Proceedings of the 2002 IEEE International Conference on Data Mining. ICDM 2003},
pages = {306--313},
title = {{Feature Selection Algorithms : A Survey and Experimental Evaluation  E  if EG edG  H  AX with distribution 2 if there exist two examples CV UD 2YEGC ` ¡ HI R P a and 2bEGDTHQ c P a that only differ in their assignment P 4SEGD ¥ H .}},
url = {https://www.cs.upc.edu/$\sim$belanche/research/R02-62.pdf%0Ahttp://www.lsi.upc.edu/$\sim$belanche/research/R02-62.pdf},
year = {2002}
}
@article{Ben-Hur2003,
abstract = {Clustering, principal component analysis, clustering stability, gene expression Clustering is one of the most commonly used tools in the analysis of gene expression data. The usage in grouping genes is based on the premise that co-expression is a result of co-regulation. It is thus a preliminary step in extracting gene networks and inference of gene function. Clustering is a form of unsupervised learning, i.e. no information on the class variable is assumed, and the objective is to find the "natural" groups in the data. However, most clustering algorithms generate a clustering even if the data has no inherent cluster structure, so external validation tools are required. The emergence of cluster structure depends on several choices: data representation and normalization, the choice of a similarity measure and clustering algorithm. In this chapter we extend the stability-based validation of cluster structure, and propose stability as a figure of merit that is useful for comparing clustering solutions, thus helping in making these choices. We use this framework to demonstrate the ability of principal component analysis (PCA) to extract features relavant to the cluster structure. We use stability as a tool for simultaneously choosing the number of principal components and the number of clusters.},
author = {Ben-Hur, Asa and Guyon, Isabelle},
doi = {10.1385/1-59259-364-x:159},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/pcachap.pdf:pdf},
issn = {1064-3745},
journal = {Functional Genomics},
number = {1},
pages = {159--182},
title = {{Detecting Stable Clusters Using Principal Component Analysis}},
year = {2003}
}
@article{Honest2020,
author = {Honest, Nirali},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Paper15_AsurveyonFeatureSelectionTechniques_June_2020.pdf:pdf},
keywords = {embedded methods,feature selection,filter methods,performance parameters,wrapper methods},
number = {September},
title = {{A Survey on Feature Selection Techniques}},
year = {2020}
}
@article{Breiman1996,
abstract = {In bagging, predictors are constructed using bootstrap samples from the training set and then aggregated to form a bagged predictor. Each bootstrap sample leaves out about 37% of the examples. These left-out examples can be used to form accurate estimates of important quantities. For instance, they can be used to give much improved estimates of node probabilities and node error rates in decision trees. Using estimated outputs instead of the observed outputs improves accuracy in regression trees. They can also be used to give nearly optimal estimates of generalization errors for bagged predictors. * Partially supported by NSF Grant 1-444063-21445 Introduction: We assume that there is a training set T= {(y n ,x n), n=1, ... ,N} and a method for constructing a predictor Q(x,T) using the given training set. The output variable y can either be a class label (classification) or numerical (regression). In bagging (Breiman[1996a]) a sequence of training sets T B,1 , ... , T B,K are generated of the same size as T by bootstrap selection from T. Then K predictors are constructed such that the kth predictor Q(x,T k,B) is based on the kth bootstrap training set. It was shown that if these predictors are aggregated--averaging in regression or voting in classification, then the resultant predictor can be considerably more accurate than the original predictor. Accuracy is increased if the prediction method is unstable, i.e. if small changes in the training set or in the parameters used in construction can result in large changes in the resulting predictor. The examples generated in Breiman[1996a] were based on trees and subset selection in regression, but it is known that neural nets are also unstable, as are other well-known prediction methods. Other methods such as nearest neighbors, are stable.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Breiman, Leo},
eprint = {arXiv:1011.1669v3},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/OOBestimation.pdf:pdf},
isbn = {9780874216561},
issn = {11758716},
journal = {New Zealand Medical Journal},
number = {1425},
pages = {97--100},
pmid = {15991970},
title = {{Out-of-Bag Estimation}},
volume = {128},
year = {1996}
}
@article{Navot2005,
abstract = {We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.},
author = {Navot, Amir and Shpigelman, Lavi and Tishby, Naftali and Vaadia, Eilon},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/NIPS-2005-nearest-neighbor-based-feature-selection-for-regression-and-its-application-to-neural-activity-Paper.pdf:pdf},
isbn = {9780262232531},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {995--1002},
title = {{Nearest neighbor based feature selection for regression and its application to neural activity}},
year = {2005}
}
@article{Guyon2005,
abstract = {The NIPS 2003 workshops included a feature selection competition organized by the authors. We provided participants with five datasets from different application domains and called for classification results using a minimal number of features. The competition took place over a period of 13 weeks and attracted 78 research groups. Participants were asked to make on-line submissions on the validation and test sets, with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop. In total 1863 entries were made on the validation sets during the development period and 135 entries on all test sets for the final competition. The winners used a combination of Bayesian neural networks with ARD priors and Dirichlet diffusion trees. Other top entries used a variety of methods for feature selection, which combined filters and/or wrapper or embedded methods using Random Forests, kernel methods, or neural networks as a classification engine. The results of the benchmark (including the predictions made by the participants and the features they selected) and the scoring software are publicly available. The benchmark is available at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions to stimulate further research.},
author = {Guyon, Isabelle and Gunn, Steve and {Ben Hur}, Asa and Dror, Gideon},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/NIPS-2004-result-analysis-of-the-nips-2003-feature-selection-challenge-Paper.pdf:pdf},
isbn = {0262195348},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Result analysis of the NIPS 2003 feature selection challenge}},
year = {2005}
}
@article{Leuven,
author = {Leuven, K U and Psychofysiologie, Neuro- and Gasthuisberg, Campus and Leuven, B-},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/mlsp_06.pdf:pdf},
title = {{COMPARISON OF TWO FEATURE EXTRACTION METHODS BASED ON MAXIMIZATION OF MUTUAL INFORMATION Nikolay Chumerin , Marc M . Van Hulle}}
}
@article{ZarPhyu2016,
abstract = {Feature Subset Selection is an essential pre-processing task in Data Mining. Feature selection process refers to choosing subset of attributes from the set of original attributes. This technique attempts to identify and remove as much irrelevant and redundant information as possible. In this paper, a new feature subset selection algorithm based on conditional mutual information approach is proposed to select the effective feature subset. The effectiveness of the proposed algorithm is evaluated by comparing with the other well-known existing feature selection algorithms using standard datasets from UC Iravine and WEKA (Waikato Environment for Knowledge Analysis). The performance of the proposed algorithm is evaluated by multi-criteria that take into account not only the classification accuracy but also number of selected features.},
author = {{Zar Phyu}, Thu and Oo, Nyein Nyein},
doi = {10.1051/matecconf20164206002},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/matecconf_iccma2016_06002.pdf:pdf},
issn = {2261236X},
journal = {MATEC Web of Conferences},
pages = {2--5},
title = {{Performance comparison of feature selection methods}},
volume = {42},
year = {2016}
}
@article{Lee2020,
abstract = {Recent studies have shown that ensemble feature selection approaches can improve the robustness and stability of final classification models. Existing methods for aggregating feature lists from different methods require use of arbitrary thresholds for selecting the top ranked features and are often based on metrics independent of the classification accuracy while selecting the optimal set. In this paper, we develop the OptSelect tool for ensemble feature selection and stability assessment of individual features for improved biomarker discovery. The software tool is packaged in R for broad dis-semination. OptSelect is a multi agent-based stochastic optimization tool designed for ensemble feature selection. Stage one involves function perturbation, where ranked list of features is generated using multiple feature selection methods. Stage two in-volves data perturbation, where feature selection is performed within randomly selected learning sets of the training da-ta. The agents are assigned to different behavior states and move according to a binary PSO algorithm. A multi-objective fitness function is used to evaluate the classification accuracy of the agents. We evaluate OptSelect system performance using the random probe method testing on five publicly available microarray datasets. The performance is compared with single feature selection techniques and existing aggregation methods. The results show that OptSelect improves classification accuracy when compared to both individual and existing rank aggregation methods. The PSO algorithm is able to uncover important discriminatory features for predicting COVID-19 disease severity, demonstrating its important role within the optSelect tool. The algorithm is incorporated into an R package and disseminated via GitHub: https://github.com/kuppa12/optSe1ect.},
author = {Lee, Eva K. and Uppal, Karan},
doi = {10.1109/BIBM49941.2020.9313269},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/lee2020.pdf:pdf},
isbn = {9781728162157},
journal = {Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020},
keywords = {agent-based,classification,ensemble feature selection,multi-objective fitness function,particle swarm optimization},
pages = {1979--1986},
title = {{OptSelect: An algorithm for ensemble feature selection and stability assessment}},
year = {2020}
}
@misc{Tibshirani1996,
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society, Series B (Methodogical),},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/LASSO_Tibshirani.pdf:pdf},
number = {1},
pages = {267--288},
title = {{Lasso Tibshirani.pdf}},
volume = {58},
year = {1996}
}
@article{Kumar2014,
abstract = {Relevant feature identification has become an essential task to apply data mining algorithms effectively in real-world scenarios. Therefore, many feature selection methods have been proposed to obtain the relevant feature or feature subsets in the literature to achieve their objectives of classification and clustering. This paper introduces the concepts of feature relevance, general procedures, evaluation criteria, and the characteristics of feature selection. A comprehensive overview, categorization, and comparison of existing feature selection methods are also done, and the guidelines are also provided for user to select a feature selection algorithm without knowing the information of each algorithm. We conclude this work with real world applications, challenges, and future research directions of feature selection.},
author = {Kumar, Vipin},
doi = {10.6029/smartcr.2014.03.007},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Kumar-Minz-2014.pdf:pdf},
journal = {The Smart Computing Review},
keywords = {classification,clustering,feature relevance,feature selection,real world applications},
number = {3},
title = {{Feature Selection: A literature Review}},
volume = {4},
year = {2014}
}
@misc{Kira1992,
author = {Kira, Kenji and Rendell, Larry},
booktitle = {The Ninth International Conference on Machine Learning},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/kira1992.pdf:pdf},
pages = {249--256},
title = {{kira - A practical approach to feature selection - 1992.pdf}},
year = {1992}
}
@article{Jimenez2017,
abstract = {Sales forecasting uses historical sales figures, in association with products characteristics and peculiarities, to predict short-term or long-term future performance in a business, and it can be used to derive sound financial and business plans. By using publicly available data, we build an accurate regression model for online sales forecasting obtained via a novel feature selection methodology composed by the application of the multi-objective evolutionary algorithm ENORA (Evolutionary NOn-dominated Radial slots based Algorithm) as search strategy in a wrapper method driven by the well-known regression model learner Random Forest. Our proposal integrates feature selection for regression, model evaluation, and decision making, in order to choose the most satisfactory model according to an a posteriori process in a multi-objective context. We test and compare the performances of ENORA as multi-objective evolutionary search strategy against a standard multi-objective evolutionary search strategy such as NSGA-II (Non-dominated Sorted Genetic Algorithm), against a classical backward search strategy such as RFE (Recursive Feature Elimination), and against the original data set.},
author = {Jim{\'{e}}nez, F. and S{\'{a}}nchez, G. and Garc{\'{i}}a, J. M. and Sciavicco, G. and Miralles, L.},
doi = {10.1016/j.neucom.2016.12.045},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/j.neucom.2016.12.045.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Feature selection,Multi-objective evolutionary algorithms,Online sales forecasting,Random forest,Regression model},
pages = {75--92},
publisher = {Elsevier},
title = {{Multi-objective evolutionary feature selection for online sales forecasting}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.12.045},
volume = {234},
year = {2017}
}
@article{Mangal2018a,
abstract = {We investigate the formation of stress hotspots in polycrystalline materials under uniaxial tensile deformation by integrating full field crystal plasticity based deformation models and machine learning techniques to gain data driven insights about microstructural properties. Synthetic 3D microstructures are created representing single phase equiaxed microstructures for generic copper alloys. Uniaxial tensile deformation is simulated using a 3-D full-field, image-based Fast Fourier Transform (FFT) technique with rate-sensitive crystal plasticity, to get local micro-mechanical fields (stress and strain rates). Stress hotspots are defined as the grains having stress values above the 90th percentile of the stress distribution. Hotspot neighborhoods are then characterized using metrics that reflect local crystallography, geometry, and connectivity. This data is used to create input feature vectors to train a random forest learning algorithm, which predicts the grains that will become stress hotspots. We are able to achieve an area under the receiving operating characteristic curve (ROC-AUC) of 0.74 for face centered cubic materials modeled on generic copper alloys. The results show the power and the limitations of the machine learning approach applied to the polycrystalline grain networks.},
archivePrefix = {arXiv},
arxivId = {1711.00118},
author = {Mangal, Ankita and Holm, Elizabeth A.},
doi = {10.1016/j.ijplas.2018.07.013},
eprint = {1711.00118},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/j.ijplas.2018.07.013.pdf:pdf},
issn = {07496419},
journal = {International Journal of Plasticity},
keywords = {Crystal plasticity,Elastic-viscoplastic material,Machine learning,Microstructures,Polycrystalline material},
pages = {122--134},
publisher = {Elsevier Ltd},
title = {{Applied machine learning to predict stress hotspots I: Face centered cubic materials}},
url = {https://doi.org/10.1016/j.ijplas.2018.07.013},
volume = {111},
year = {2018}
}
@article{Vafaie1994,
author = {Vafaie, Haleh and {De Jong}, Kenneth},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Improving_A_Rule_Induction_System_Using_Genetic_Al.pdf:pdf},
journal = {Machine Learning. A Multistrategy Approach.},
number = {November},
pages = {453--471},
title = {{Improving a Rule Induction System Using Genetic Algorithms}},
volume = {IV},
year = {1994}
}
@article{Ladha2011,
abstract = {Feature selection is an important topic in data mining, especially for high dimensional datasets. Feature selection (also known as subset selection) is a process commonly used in machine learning, wherein subsets of the features available from the data are selected for application of a learning algorithm. The best subset contains the least number of dimensions that most contribute to accuracy; we discard the remaining, unimportant dimensions. This is an important stage of preprocessing and is one of two ways of avoiding the curse of dimensionality (the other is feature extraction). There are two approaches in Feature selection known as Forward selection and backward selection. Feature selection has been an active research area in pattern recognition, statistics, and data mining communities. The main idea of feature selection is to choose a subset of input variables by eliminating features with little or no predictive information. Feature selection methods can be decomposed into three broad classes. One is Filter methods and another one is Wrapper method and the third one is Embedded method. This paper presents an empirical comparison of feature selection methods and its algorithms. In view of the substantial number of existing feature selection algorithms, the need arises to count on criteria that enable to adequately decide which algorithm to use in certain situations. This work reviews several fundamental algorithms found in the literature and assesses their performance in a controlled scenario.},
author = {Ladha, L and Deepa, T},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/IJCSE11-03-05-051.pdf:pdf},
isbn = {0975-3397},
journal = {International Journal on Computer Science and Engineering},
keywords = {- feature selection,Feature Selection Algorithms,Feature Selection Methods,feature selection,feature selection algorithms,feature selection methods},
number = {5},
pages = {1787--1797},
title = {{Feature Selection Methods And Algorithms}},
url = {http://journals.indexcopernicus.com/abstract.php?icid=945099},
volume = {3},
year = {2011}
}
@article{Nationala,
abstract = {Over the last two decades, scanning transmission electron microscopy (STEM) has become a very popular and widespread technique, with the number of publications and presentations making use of STEM techniques increasing by about an order of magnitude. Although the strengths of the technique for providing high-resolution structural and analytical information have been known and understood for much longer than that, the key to its more recent popularity has undoubtedly been the availability of STEM modes on instruments available from the major TEM manufacturers. Gone are the days when researchers want- ing the unique capabilities of high-resolution STEM had to undertake the task of keeping a VG dedicated STEM instrument operating. Given the current interest in the technique, we felt that the time was right to review the current state of knowledge about STEM and STEM-related techniques and their application to a range of materials problems. The purpose of this volume is both to educate those who wish to deepen their understanding of STEM and to inform those who are seeking a review of the latest applications and methods associated with STEM. We are delighted that so many of our colleagues accepted our invitation to contribute to this volume, and we are indebted to them for their efforts in creating such excellent contributions. The follow- ing chapters illustrate how close STEM has brought us to the ultimate materials characterisation challenge of analysing materials atom by atom. We hope that the following chapters demonstrate the spectacular results that can be achieved when performing the relatively simple experiment of focusing a beam of electrons down to an atomic scale and measuring the scattering that results. Stephen},
author = {National, Gross and Pillars, Happiness},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Koller+Sahami ICML96.pdf:pdf},
title = {{No 主観的健康感を中心とした在宅高齢者における 健康関連指標に関する共分散構造分析Title}}
}
@article{Iguyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods. {\textcopyright} 2003 Isabelle Guyon and Andre Elisseeff.},
author = {Iguyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1162/153244303322753616},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/guyon03a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Bioinformatics,Clustering,Computational biology,Feature selection,Filters,Gene expression,Genomics,Information retrieval,Information theory,Microarray,Model selection,Pattern discovery,Proteomics,QSAR,Space dimensionality reduction,Statistical testing,Support vector machines,Text classification,Variable selection,Wrappers},
pages = {1157--1182},
title = {{An introduction to variable and feature selection}},
volume = {3},
year = {2003}
}
@article{Hall1995,
abstract = {Feature selection is often an essential data processing step prior to applying a learning algorithm. The re- moval of irrelevant and redundant information often improves the performance of machine learning algo- rithms. There are two common approaches: a wrapper uses the intended learning algorithm itself to evaluate the usefulness of features, while a filter evaluates fea- tures according to heuristics based on general charac- teristics of the data. The wrapper approach is generally considered to produce better feature subsets but runs much more slowly than a filter. This paper describes a new filter approach to feature selection that uses a correlation based heuristic to evaluate the worth of fea- ture subsets When applied as a data preprocessing step for two common machine learning algorithms, the new method compares favourably with the wrapper but re- quires much less computation.},
author = {Hall, Mark A and Smith, Lloyd A},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/FLAIRS99-042.pdf:pdf},
journal = {FLAIRS conference},
pages = {235--239},
title = {{Feature Selection for Machine Learning: Comparing a Correlation-based Filter Approach to the Wrapper}},
year = {1995}
}
@article{Mangal2018,
abstract = {The first step in constructing a machine learning model is defining the features of the dataset that can be used for optimal learning. In this work, we discuss feature selection methods, which can be used to build better models, as well as achieve model interpretability. We applied these methods in the context of stress hotspot classification problem, to determine what microstructural characteristics can cause stress to build up in certain grains during uniaxial tensile deformation. The results show how some feature selection techniques are biased and demonstrate a preferred technique to get feature rankings for physical interpretations.},
archivePrefix = {arXiv},
arxivId = {1804.09604},
author = {Mangal, Ankita and Holm, Elizabeth A.},
doi = {10.1007/s40192-018-0109-8},
eprint = {1804.09604},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Feature_Imp_Final.pdf:pdf},
issn = {21939772},
journal = {Integrating Materials and Manufacturing Innovation},
keywords = {Crystal plasticity,Feature selection,Machine learning,Random forests,Stress hotspots,Titanium alloys},
number = {3},
pages = {87--95},
title = {{A Comparative Study of Feature Selection Methods for Stress Hotspot Classification in Materials}},
volume = {7},
year = {2018}
}
@article{QinbaoSong;JingjieNi;GuangtaoWang2014,
author = {{Qinbao Song; Jingjie Ni; Guangtao Wang}},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/FAST.pdf:pdf},
issn = {2321-9653},
journal = {International Journal for Research in Applied Science and Engineering Technology},
number = {XI},
pages = {47--50},
title = {{A FAST Algorithm for High Dimensional Data using Clustering-Based Feature Subset Selection}},
volume = {2},
year = {2014}
}
@article{Ruyssinck2014,
abstract = {One of the long-standing open challenges in computational systems biology is the topology inference of gene regulatory networks from high-throughput omics data. Recently, two community-wide efforts, DREAM4 and DREAM5, have been established to benchmark network inference techniques using gene expression measurements. In these challenges the overall top performer was the GENIE3 algorithm. This method decomposes the network inference task into separate regression problems for each gene in the network in which the expression values of a particular target gene are predicted using all other genes as possible predictors. Next, using tree-based ensemble methods, an importance measure for each predictor gene is calculated with respect to the target gene and a high feature importance is considered as putative evidence of a regulatory link existing between both genes. The contribution of this work is twofold. First, we generalize the regression decomposition strategy of GENIE3 to other feature importance methods. We compare the performance of support vector regression, the elastic net, random forest regression, symbolic regression and their ensemble variants in this setting to the original GENIE3 algorithm. To create the ensemble variants, we propose a subsampling approach which allows us to cast any feature selection algorithm that produces a feature ranking into an ensemble feature importance algorithm. We demonstrate that the ensemble setting is key to the network inference task, as only ensemble variants achieve top performance. As second contribution, we explore the effect of using rankwise averaged predictions of multiple ensemble algorithms as opposed to only one. We name this approach NIMEFI (Network Inference using Multiple Ensemble Feature Importance algorithms) and show that this approach outperforms all individual methods in general, although on a specific network a single method can perform better. An implementation of NIMEFI has been made publicly available. {\textcopyright} 2014 Ruyssinck et al.},
author = {Ruyssinck, Joeri and Huynh-Thu, V{\^{a}}n Anh and Geurts, Pierre and Dhaene, Tom and Demeester, Piet and Saeys, Yvan},
doi = {10.1371/journal.pone.0092709},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/e9eefccf8778eea471176e37bd22d82dc04c.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {3},
pages = {1--13},
pmid = {24667482},
title = {{NIMEFI: Gene regulatory network inference using multiple ensemble feature importance algorithms}},
volume = {9},
year = {2014}
}
@article{Vafaie1993,
abstract = {Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency.},
author = {Vafaie, Haleh and {De Jong}, Kenneth},
doi = {10.1109/TAI.1993.633981},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/download-1.pdf:pdf},
isbn = {0818642009},
issn = {10636730},
journal = {Proceedings of the International Conference on Tools with Artificial Intelligence},
number = {November},
pages = {356--363},
title = {{Robust feature selection algorithms}},
year = {1993}
}
@article{DeJong1988,
abstract = {Genetic algorithms represent a class of adaptive search techniques that have been intensively studied in recent years. Much of the interest in genetic algorithms is due to the fact that they provide a set of efficient domain-independent search heuristics which are a significant improvement over traditional “weak methods” without the need for incorporating highly domain-specific knowledge. There is now considerable evidence that genetic algorithms are useful for global function optimization and NP-hard problems. Recently, there has been a good deal of interest in using genetic algorithms for machine learning problems. This paper provides a brief overview of how one might use genetic algorithms as a key element in learning systems. {\textcopyright} 1988, Kluwer Academic Publishers. All rights reserved.},
author = {de Jong, Kenneth},
doi = {10.1023/A:1022606120092},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/DeJong1988_Article_LearningWithGeneticAlgorithmsA.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Genetic algorithms,classifier systems,competition-based learning,learning task programs},
number = {2},
pages = {121--138},
title = {{Learning with Genetic Algorithms: An Overview}},
volume = {3},
year = {1988}
}
@article{Hauskrecht2011,
author = {Hauskrecht, Milos and Pelikan, Richard and Valko, Michal and Lyons-weiler, James and Hauskrecht, Milos and Pelikan, Richard and Valko, Michal and Selection, James Lyons-weiler Feature and Dubitzky, Proteomics Werner and Granzow, Martin},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/chapter-Hauskrecht.pdf:pdf},
isbn = {9780387475097},
title = {{Feature Selection and Dimensionality Reduction in Genomics and Proteomics To cite this version : HAL Id : hal-00643496 Chapter 1}},
year = {2011}
}
@article{Chandrashekar2014,
abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Chandrashekar - Feature Selection Methods - 2014.pdf:pdf},
issn = {00457906},
journal = {Computers and Electrical Engineering},
number = {1},
pages = {16--28},
publisher = {Elsevier Ltd},
title = {{A survey on feature selection methods}},
url = {http://dx.doi.org/10.1016/j.compeleceng.2013.11.024},
volume = {40},
year = {2014}
}
@article{Huynh-Thu2012,
abstract = {Motivation: Univariate statistical tests are widely used for biomarker discovery in bioinformatics. These procedures are simple, fast and their output is easily interpretable by biologists but they can only identify variables that provide a significant amount of information in isolation from the other variables. As biological processes are expected to involve complex interactions between variables, univariate methods thus potentially miss some informative biomarkers. Variable relevance scores provided by machine learning techniques, however, are potentially able to highlight multivariate interacting effects, but unlike the p-values returned by univariate tests, these relevance scores are usually not statistically interpretable. This lack of interpretability hampers the determination of a relevance threshold for extracting a feature subset from the rankings and also prevents the wide adoption of these methods by practicians. Results: We evaluated several, existing and novel, procedures that extract relevant features from rankings derived from machine learning approaches. These procedures replace the relevance scores with measures that can be interpreted in a statistical way, such as p-values, false discovery rates, or family wise error rates, for which it is easier to determine a significance level. Experiments were performed on several artificial problems as well as on real microarray datasets. Although the methods differ in terms of computing times and the tradeoff, they achieve in terms of false positives and false negatives, some of them greatly help in the extraction of truly relevant biomarkers and should thus be of great practical interest for biologists and physicians. As a side conclusion, our experiments also clearly highlight that using model performance as a criterion for feature selection is often counter-productive. {\textcopyright} The Author 2012. Published by Oxford University Press. All rights reserved.},
author = {Huynh-Thu, V{\^{a}}n Anh and Saeys, Yvan and Wehenkel, Louis and Geurts, Pierre},
doi = {10.1093/bioinformatics/bts238},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/Bioinformatics-2012-Huynh-Thu-1766-74.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {13},
pages = {1766--1774},
pmid = {22539669},
title = {{Statistical interpretation of machine learning-based feature importance scores for biomarker discovery}},
volume = {28},
year = {2012}
}
@article{Verma2020,
abstract = {As detailed in Chap. 4, features have been extracted from the pre-processed data. Too many features may lead to the curse of dimensionality issues. This chapter explainshow to obtain aset of relevant features which is the process of feature selection. Feature selection is the process in which most informative variables are selected for the generation of the model. It helps to remove the redundant data and contributes toproper classification. While minimizing the redundancy, we should keep in mind that the predicted information must be preserved as much as possible. This chapter describes various feature selection methods such asPrincipalComponentAnalysis (PCA)-based approach,MutualInformation (MI), BhattacharyyaDistance (BD), and IndependentComponentAnalysis (ICA). A novel feature selection based on graphical reviewhas also been elaborated later in this chapter.},
author = {Verma, Nishchal K. and Salour, Al},
doi = {10.1007/978-981-15-0512-6_5},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/AAAI92-020.pdf:pdf},
issn = {21984190},
journal = {Studies in Systems, Decision and Control},
pages = {175--200},
title = {{Feature selection}},
volume = {256},
year = {2020}
}
@article{Hall1991,
author = {Hall, Dearborn},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/AAAI91-085.pdf:pdf},
pages = {547--552},
title = {{From: AAAI - 91 Proceedings. Copyright {\textcopyright} 1991 , AAAI (www.aaai.org). All rights reserved.}},
volume = {3},
year = {1991}
}
@article{Abusamra2013,
abstract = {Microarray gene expression data gained great importance in recent years due to its role in disease diagnoses and prognoses which help to choose the appropriate treatment plan for patients. This technology has shifted a new era in molecular classification. Interpreting gene expression data remains a difficult problem and an active research area due to their native nature of "high dimensional low sample size". Such problems pose great challenges to existing classification methods. Thus, effective feature selection techniques are often needed in this case to aid to correctly classify different tumor types and consequently lead to a better understanding of genetic signatures as well as improve treatment strategies. This paper aims on a comparative study of state-of-the-art feature selection methods, classification methods, and the combination of them, based on gene expression data. We compared the efficiency of three different classification methods including: support vector machines, k-nearest neighbor and random forest, and eight different feature selection methods, including: information gain, twoing rule, sum minority, max minority, gini index, sum of variances, t-statistics, and one-dimension support vector machine. Five-fold cross validation was used to evaluate the classification performance. Two publicly available gene expression data sets of glioma were used in the experiments. Results revealed the important role of feature selection in classifying gene expression data. By performing feature selection, the classification accuracy can be significantly boosted by using a small number of genes. The relationship of features selected in different feature selection methods is investigated and the most frequent features selected in each fold among all methods for both datasets are evaluated. {\textcopyright} 2013 The Authors.},
author = {Abusamra, Heba},
doi = {10.1016/j.procs.2013.10.003},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/A_Comparative_Study_of_Feature_Selection_and_Class.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Classification,Feature selection,Gene expression,Glioma,Microarray data},
pages = {5--14},
title = {{A comparative study of feature selection and classification methods for gene expression data of glioma}},
volume = {23},
year = {2013}
}
@article{Stijven2011,
abstract = {Feature selection in high-dimensional data sets is an open problem with no universal satisfactory method available. In this paper we discuss the requirements for such a method with respect to the various aspects of feature importance and explore them using regression random forests and symbolic regression. We study 'conventional' feature selection with both methods on several test problems and a case study, compare the results, and identify the conceptual differences in generated feature importances. We demonstrate that random forests might overlook important variables (significantly related to the response) for various reasons, while symbolic regression identifies all important variables if models of sufficient quality are found. We explain the results by the fact that variable importances obtained by these methods have different semantics. {\textcopyright} 2011 ACM.},
author = {Stijven, Sean and Minnebo, Wouter and Vladislavleva, Katya},
doi = {10.1145/2001858.2002059},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/2001858.2002059.pdf:pdf},
isbn = {9781450306904},
journal = {Genetic and Evolutionary Computation Conference, GECCO'11 - Companion Publication},
keywords = {feature selection,genetic programming,random forests,symbolic regression,variable importance,variable selection},
pages = {623--630},
title = {{Separating the wheat from the chaff: On feature selection and feature importance in regression random forests and symbolic regression}},
year = {2011}
}
@article{Jovic2015,
abstract = {Feature selection (FS) methods can be used in data pre-processing to achieve efficient data reduction. This is useful for finding accurate data models. Since exhaustive search for optimal feature subset is infeasible in most cases, many search strategies have been proposed in literature. The usual applications of FS are in classification, clustering, and regression tasks. This review considers most of the commonly used FS techniques. Particular emphasis is on the application aspects. In addition to standard filter, wrapper, and embedded methods, we also provide insight into FS for recent hybrid approaches and other advanced topics.},
author = {Jovi{\'{c}}, A. and Brki{\'{c}}, K. and Bogunovi{\'{c}}, N.},
doi = {10.1109/MIPRO.2015.7160458},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/763354.MIPRO_2015_JovicBrkicBogunovic.pdf:pdf},
isbn = {9789532330854},
journal = {2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2015 - Proceedings},
pages = {1200--1205},
title = {{A review of feature selection methods with applications}},
year = {2015}
}
@article{Yao2020,
abstract = {Feature selection often leads to increased model interpretability, faster computation, and improved model performance by discarding irrelevant or redundant features. While feature selection is a well-studied problem with many widely-used techniques, there are typically two key challenges: i) many existing approaches become computationally intractable in huge-data settings with millions of observations and features; and ii) the statistical accuracy of selected features degrades in high-noise, high-correlation settings, thus hindering reliable model interpretation. We tackle these problems by proposing Stable Minipatch Selection (STAMPS) and Adaptive STAMPS (AdaSTAMPS). These are meta-algorithms that build ensembles of selection events of base feature selectors trained on many tiny, (adaptively-chosen) random subsets of both the observations and features of the data, which we call minipatches. Our approaches are general and can be employed with a variety of existing feature selection strategies and machine learning techniques. In addition, we provide theoretical insights on STAMPS and empirically demonstrate that our approaches, especially AdaSTAMPS, dominate competing methods in terms of feature selection accuracy and computational time.},
archivePrefix = {arXiv},
arxivId = {2010.08529},
author = {Yao, Tianyi and Allen, Genevera I.},
eprint = {2010.08529},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/2010.08529.pdf:pdf},
pages = {1--22},
title = {{Feature Selection for Huge Data via Minipatch Learning}},
url = {http://arxiv.org/abs/2010.08529},
year = {2020}
}
@article{Donick2021,
abstract = {Conventionally, random forests are built from “greedy” decision trees which each consider only one split at a time during their construction. The sub-optimality of greedy implementation has been well-known, yet mainstream adoption of more sophisticated tree building algorithms has been lacking. We examine under what circumstances an implementation of less greedy decision trees actually yields outperformance. To this end, a “stepwise lookahead” variation of the random forest algorithm is presented for its ability to better uncover binary feature interdependencies. In contrast to the greedy approach, the decision trees included in this random forest algorithm, each simultaneously consider three split nodes in tiers of depth two. It is demonstrated on synthetic data and financial price time series that the lookahead version significantly outperforms the greedy one when (a) certain non-linear relationships between feature-pairs are present and (b) if the signal-to-noise ratio is particularly low. A long-short trading strategy for copper futures is then backtested by training both greedy and stepwise lookahead random forests to predict the signs of daily price returns. The resulting superior performance of the lookahead algorithm is at least partially explained by the presence of “XOR-like” relationships between long-term and short-term technical indicators. More generally, across all examined datasets, when no such relationships between features are present, performance across random forests is similar. Given its enhanced ability to understand the feature-interdependencies present in complex systems, this lookahead variation is a useful extension to the toolkit of data scientists, in particular for financial machine learning, where conditions (a) and (b) are typically met.},
archivePrefix = {arXiv},
arxivId = {2009.14572},
author = {Donick, Delilah and Lera, Sandro Claudio},
doi = {10.1038/s41598-021-88571-3},
eprint = {2009.14572},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/2009.14572.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--18},
pmid = {33927260},
title = {{Uncovering feature interdependencies in high-noise environments with stepwise lookahead decision forests}},
volume = {11},
year = {2021}
}
@article{Liu1997,
abstract = {Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant and/or redundant attributes. Chi2 is a simple and general algorithm that uses the $\chi$ 2 statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data. It achieves feature selection via discretization. It can handle mixed attributes, work with multiclass data, and remove irrelevant and redundant attributes. {\textcopyright} 1997 IEEE.},
author = {Liu, Huan and Setiono, Rudy},
doi = {10.1109/69.617056},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/1997-Liu-IEEETKDE.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Discretization,Feature selection,Pattern classification},
number = {4},
pages = {642--645},
title = {{Feature selection via discretization}},
volume = {9},
year = {1997}
}
@article{Gregorutti2017,
abstract = {This paper is about variable selection with the random forests algorithm in presence of correlated predictors. In high-dimensional regression or classification frameworks, variable selection is a difficult task, that becomes even more challenging in the presence of highly correlated predictors. Firstly we provide a theoretical study of the permutation importance measure for an additive regression model. This allows us to describe how the correlation between predictors impacts the permutation importance. Our results motivate the use of the recursive feature elimination (RFE) algorithm for variable selection in this context. This algorithm recursively eliminates the variables using permutation importance measure as a ranking criterion. Next various simulation experiments illustrate the efficiency of the RFE algorithm for selecting a small number of variables together with a good prediction error. Finally, this selection algorithm is tested on the Landsat Satellite data from the UCI Machine Learning Repository.},
archivePrefix = {arXiv},
arxivId = {1310.5726},
author = {Gregorutti, Baptiste and Michel, Bertrand and Saint-Pierre, Philippe},
doi = {10.1007/s11222-016-9646-1},
eprint = {1310.5726},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/1310.5726.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Random forests,Supervised learning,Variable importance,Variable selection},
number = {3},
pages = {659--678},
title = {{Correlation and variable importance in random forests}},
volume = {27},
year = {2017}
}
@article{Benesty2009,
abstract = {Having discussed different error criteria and performance measures, we now begin our search for reliable and practical noise reduction filters. In this chapter, we restrict our attention to filters in the time domain. Particularly, much emphasis is on the Wiener filter [125] as it is directly derived from the MSE criterion and most well-known algorithms are somehow related to it.},
author = {Benesty, Jacob and Chen, Jingdong and Huang, Yiteng and Cohen, Israel},
doi = {10.1007/978-3-642-00296-0_7},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/978-3-642-00296-0_5.pdf:pdf},
isbn = {9783642002960},
issn = {18662617},
journal = {Springer Topics in Signal Processing},
keywords = {Clean Speech,Noise Reduction,Optimal Filter,Speech Signal,Wiener Filter},
pages = {1--18},
title = {{Optimal filters in the time domain}},
volume = {2},
year = {2009}
}
@article{Hall2000,
abstract = {Algorithms for feature selection fall into two broad categories: wrappers that use the learning algorithm itself to evaluate the usefulness of features and filters that evaluate features according to heuristics based on general characteristics of the data. For application to large databases, filters have proven to be more practical than wrappers because they are much faster. However, most existing filter algorithms only work with discrete classification problems. This paper describes a fast, correlation-based filter algorithm that can be applied to continuous and discrete problems. The algorithm often outperforms the well-known ReliefF attribute estimator when used as a preprocessing step for naive Bayes, instance-based learning, decision trees, locally weighted regression, and model trees. It performs more feature selection than ReliefF doesreducing the data dimensionality by fifty percent in most cases. Also, decision and model trees built from the preprocessed data are often significantly smaller. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hall, Mark A},
eprint = {arXiv:1011.1669v3},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/99MH-Feature-Select.pdf:pdf},
isbn = {1-55860-707-2},
issn = {1170-487X},
journal = {Machine Learning Proc Seventeenth International conference on Machine Learning},
keywords = {feature selection,machine learning},
pages = {1--16},
pmid = {25246403},
title = {{Feature Selection for Discrete and Numeric Class Machine Learning 1 Introduction}},
year = {2000}
}
@article{Hamer2021,
abstract = {Current feature selection methods, especially applied to high dimensional data, tend to suffer from instability since marginal modifications in the data may result in largely distinct selected feature sets. Such instability strongly limits a sound interpretation of the selected variables by domain experts. Defining an adequate stability measure is also a research question. In this work, we propose to incorporate into the stability measure the importances of the selected features in predictive models. Such feature importances are directly proportional to feature weights in a linear model. We also consider the generalization to a non-linear setting. We illustrate, theoretically and experimentally, that current stability measures are subject to undesirable behaviors, for example, when they are jointly optimized with predictive accuracy. Results on micro-array and mass-spectrometric data show that our novel stability measure corrects for overly optimistic stability estimates in such a bi-objective context, which leads to improved decision-making. It is also shown to be less prone to the underor over-estimation of the stability value in feature spaces with groups of highly correlated variables.},
author = {Hamer, Victor and Dupont, Pierre},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/20-366.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Bi-objective optimization,Bioinformatics,Feature importance,Feature selection,Selection stability},
pages = {1--57},
title = {{An importance weighted feature selection stability measure}},
volume = {22},
year = {2021}
}
@article{Huang2015,
abstract = {Supervised feature selection research has a long history. Its popularity exploded in the past 30 years due to the advance ofinformation technology and the need to analyze high-dimensional data sets. Research papers published during these yearswere mostly from the machine learning and artificial intelligence community. The emphasis was largely on improving modelaccuracy using empirical methods; whereas the issue of feature relevance was somewhat overlooked. Feature selection methodswere loosely classified as filters, wrappers, and embedded methods with little attention paid to their intricate details. This paperprovides a tutorial of supervised feature selection, on the basis of reviewing frequently cited papers in this area and a numberof classical publications from the statistics community. The objective of feature selection (either to improve model predictiveaccuracy or to determine relevance for hypothesis generation) is presented and discussed in details. Various supervised featureselection methods are classified using a detailed taxonomy. Guidelines for using feature selection methods in practice areprovided based on a comprehensive review of the performance of these methods. Issues that require further attention are alsodiscussed.},
author = {Huang, Samuel H.},
doi = {10.5430/air.v4n2p22},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/6218-21898-1-PB.pdf:pdf},
issn = {1927-6974},
journal = {Artificial Intelligence Research},
keywords = {embedded methods,feature selection,filters,redundancy,relevance,wrappers},
number = {2},
title = {{Supervised feature selection: A tutorial}},
volume = {4},
year = {2015}
}
@article{Liu1996,
abstract = {Feature selection can be defined as a problem of finding a minimum set of M relevant attributes that describes the dataset as well as the original N attributes do, where M N . After examining the problems with both the exhaustive and the heuristic approach to feature selection, this paper proposes a probabilistic approach. The theoretic analysis and the experimental study show that the proposed approach is simple to implement and guaranteed to find the optimal if resources permit. It is also fast in obtaining results and effective in selecting features that improve the performance of a learning algorithm. An on-site application involving huge datasets has been conducted independently. It proves the effectiveness and scalability of the proposed algorithm. Discussed also are various aspects and applications of this feature selection algorithm. 1 Introduction The problem of feature selection can be defined as finding M relevant attributes among the N original attrib...},
author = {Liu, H and Setiono, R},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/10.1.1.294.9980.pdf:pdf},
journal = {Proceedings of International Conference on Machine Learning},
pages = {319--327},
title = {{A probabilistic approach to feature selection. A filter solution}},
year = {1996}
}
@article{Motoda2002,
abstract = {F atur e esele ction is a pr c o ess that cho oses a subset of fe atur es fr om the original fe atur es so that the fe a? tur esp ac e is optimally r duc dac or e e c ding to a c ertain criterion? F atur e e extr action?c onstruction is a pr c o ess thr ough which a set of new fe atur es is cr ate e d? They ar e use d either in isolation or in c ombination? A ll attempt to impr ove p erformanc e such as estimate d ac? cur acy? visualization and c ompr ehensibility of le arne d knowle dge? Basic appr aches to these thr ear o e er e? viewe d giving p ointers to r efer enc es for further stud? ies?},
author = {Motoda, Hiroshi and Liu, Huan},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/10.1.1.115.3627.pdf:pdf},
isbn = {ISBN:0792381963},
journal = {Communication of IICM},
pages = {67--72},
title = {{Feature selection, extraction and construction}},
volume = {5},
year = {2002}
}
@article{Gilad-Bachrach2004,
abstract = {Feature selection is the task of choosing a small set out of a given set of features that capture the relevant properties of the data. In the context of supervised classification problems the relevance is determined by the given labels on the training data. A good choice of features is a key for building compact and accurate classifiers. In this paper we introduce a margin based feature selection criterion and apply it to measure the quality of sets of features. Using margins we devise novel selection algorithms for multi-class classification problems and provide theoretical generalization bound. We also study the well known Relief algorithm and show that it resembles a gradient ascent over our margin criterion. We apply our new algorithm to various datasets and show that our new Simba algorithm, which directly optimizes the margin, outperforms Relief.},
author = {Gilad-Bachrach, Ran and Navot, Amir and Tishby, Naftali},
doi = {10.1145/1015330.1015352},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/10.1.1.75.3755.pdf:pdf},
isbn = {1581138385},
journal = {Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004},
pages = {337--344},
title = {{Margin based feature selection - Theory and algorithms}},
year = {2004}
}
@article{Ferri1994,
abstract = {The combinatorial search problem arising in feature selection in high dimensional spaces is considered. Recently developed techniques based on the classical sequential methods and the (l, r) search called Floating search algorithms are compared against the Genetic approach to feature subset search. Both approaches have been designed with the view to give a good compromise between efficiency and effectiveness for large problems. The purpose of this paper is to investigate the applicability of these techniques to high dimensional problems of feature selection. The aim is to establish whether the properties inferred for these techniques from medium scale experiments involving up to a few tens of dimensions extend to dimensionalities of one order of magnitude higher. Further, relative merits of these techniques vis-a-vis such high dimensional problems are explored and the possibility of exploiting the best aspects of these methods to create a composite feature selection procedure with superior properties is considered. {\textcopyright} 1994, Elsevier Science & Technology. All rights reserved.},
author = {Ferri, F. J. and Pudil, P. and Hatef, M. and Kittler, J.},
doi = {10.1016/B978-0-444-81892-8.50040-7},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/10.1.1.24.4369.pdf:pdf},
issn = {09230459},
journal = {Machine Intelligence and Pattern Recognition},
number = {C},
pages = {403--413},
title = {{Comparative study of techniques for large-scale feature selection}},
volume = {16},
year = {1994}
}
@article{Karthikayani2020,
abstract = {The major advancing techniques in machine learning are mainly two, they are deep learning and computer vision. The advanced deep learning techniques are highly promising to increase the interest in research within the upcoming years. This is often because the eminent benefits in overcoming the drawbacks within the outdated techniques for producing the result accurately. The theme of this paper is to provide a comprehensive description on the convolution neural network and its recent improvements which includes the CNN - S convolution neural network segmentation, CNN - CBIR convolution neural network - content-based image retrieval system. This survey paper provides a detailed summary within the latest advancements in the domain of CNN with various extended applications through its classification for improved understanding. Analysing the performance is done considering the speed, accuracy and ease.},
author = {Karthikayani, K. and Arunachalam, A. R.},
doi = {10.1063/5.0028564},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/5.0028564.pdf:pdf},
isbn = {9780735440128},
issn = {15517616},
journal = {AIP Conference Proceedings},
keywords = {CNN – CBIR,CNN – S,deep learning,neural network},
number = {October},
title = {{A survey on deep learning feature extraction techniques}},
volume = {2282},
year = {2020}
}
@article{Blum1997,
abstract = {In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.},
author = {Blum, Avrim L. and Langley, Pat},
file = {:home/lk/Documents/Papers/MachineLearning/FeatureExtraction/1-s2.0-S0004370297000635-main.pdf:pdf},
journal = {Artificial Intelligence},
keywords = {machine learning,relevant examples,relevant features},
number = {1-2},
pages = {245--271},
title = {{Artificial Intelligence Selection of relevant features and examples in machine}},
volume = {97},
year = {1997}
}
